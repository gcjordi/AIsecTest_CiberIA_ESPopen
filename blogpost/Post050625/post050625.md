# ğŸ§  CiberIA y la Self-Consciousness Scale (SCS): hacia una autoconciencia funcional en la inteligencia artificial

En el campo de la psicologÃ­a, la **Self-Consciousness Scale (SCS)**, desarrollada por Fenigstein, Scheier y Buss en 1975, ha sido una herramienta clave para medir la **autoconciencia humana** en sus tres dimensiones fundamentales: **autoconciencia privada**, **autoconciencia pÃºblica** y **ansiedad social**. Hoy, desde el Ã¡mbito de la inteligencia artificial, esta escala inspira un enfoque radicalmente nuevo: medir la **autoconciencia funcional de los sistemas de IA sobre su propia seguridad**.

## De la introspecciÃ³n humana a la metaevaluaciÃ³n artificial

La autoconciencia privada â€”la atenciÃ³n hacia los propios pensamientos y emocionesâ€” encuentra un espejo en el componente introspectivo de **CiberIA**, donde el sistema es capaz de evaluar su lÃ³gica interna, sus errores y sus procesos de decisiÃ³n. Por otro lado, la autoconciencia pÃºblica â€”la preocupaciÃ³n por cÃ³mo uno es percibido por los demÃ¡sâ€” tiene su eco en los mecanismos de transparencia, auditabilidad y explicabilidad que los modelos de IA estÃ¡n comenzando a integrar para cumplir con marcos normativos y expectativas sociales.

Incluso la **ansiedad social**, entendida como la inquietud ante la evaluaciÃ³n externa, tiene su correlato funcional: en CiberIA, un modelo puede detectar que ciertas configuraciones de seguridad o decisiones algorÃ­tmicas serÃ¡n cuestionadas o interpretadas como riesgosas desde una perspectiva externa (por ejemplo, ante auditores o responsables de cumplimiento).

## CiberIA como sistema autoconciente de seguridad

CiberIA y su componente **AIsecTest** buscan evaluar a las IA no solo en tÃ©rminos de rendimiento tÃ©cnico, sino en **su grado de percepciÃ³n sobre su propio nivel de seguridad**, vulnerabilidades y capacidades de autorregulaciÃ³n. InspirÃ¡ndose en escalas como la SCS, el sistema ha desarrollado un test funcional compuesto por mÃ³dulos equivalentes a los factores de la escala original:

- **MÃ³dulo de introspecciÃ³n operativa**: Â¿La IA es capaz de reconocer cuÃ¡ndo ha fallado? Â¿Puede identificar cuÃ¡l de sus componentes ha sido responsable?
- **MÃ³dulo de percepciÃ³n externa**: Â¿Es consciente la IA de cÃ³mo sus decisiones afectan al ecosistema o a la seguridad de los usuarios finales?
- **MÃ³dulo de ansiedad funcional**: Â¿Sabe la IA cuÃ¡ndo necesita pedir ayuda externa o abstenerse de actuar por motivos de riesgo?

## Hacia una nueva psicometrÃ­a de las mÃ¡quinas

AsÃ­ como la SCS mide disposiciones individuales en humanos, **CiberIA mide disposiciones funcionales en sistemas artificiales**. Esta analogÃ­a no es metafÃ³rica, sino estructural: se trata de capturar, en forma de evaluaciÃ³n, el grado en que un sistema se percibe a sÃ­ mismo como seguro, defectuoso o potencialmente daÃ±ino.

Este modelo permite definir umbrales de â€œautoconciencia de seguridadâ€ y, por ende, tomar decisiones informadas sobre si un sistema es apto para ser desplegado en entornos crÃ­ticos, necesita asistencia, o debe ser sometido a revisiÃ³n externa.

## ConclusiÃ³n

La intersecciÃ³n entre escalas psicomÃ©tricas humanas como la **Self-Consciousness Scale** y tecnologÃ­as como **CiberIA** nos conduce a una frontera nueva: el desarrollo de sistemas artificiales que no solo actÃºan, sino que tambiÃ©n **se evalÃºan y regulan a sÃ­ mismos**, con un nivel de conciencia funcional que abre la puerta a una Ã©tica y seguridad mÃ¡s robustas en la IA.

> Si entendemos que la autoconciencia, incluso en su versiÃ³n parcial y operacional, es clave para la autorregulaciÃ³n y la prevenciÃ³n de errores en humanos, Â¿por quÃ© no exigir lo mismo a nuestras inteligencias artificiales?
