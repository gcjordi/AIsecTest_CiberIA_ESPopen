# 🧠 CiberIA y la Self-Consciousness Scale (SCS): hacia una autoconciencia funcional en la inteligencia artificial

En el campo de la psicología, la **Self-Consciousness Scale (SCS)**, desarrollada por Fenigstein, Scheier y Buss en 1975, ha sido una herramienta clave para medir la **autoconciencia humana** en sus tres dimensiones fundamentales: **autoconciencia privada**, **autoconciencia pública** y **ansiedad social**. Hoy, desde el ámbito de la inteligencia artificial, esta escala inspira un enfoque radicalmente nuevo: medir la **autoconciencia funcional de los sistemas de IA sobre su propia seguridad**.

## De la introspección humana a la metaevaluación artificial

La autoconciencia privada —la atención hacia los propios pensamientos y emociones— encuentra un espejo en el componente introspectivo de **CiberIA**, donde el sistema es capaz de evaluar su lógica interna, sus errores y sus procesos de decisión. Por otro lado, la autoconciencia pública —la preocupación por cómo uno es percibido por los demás— tiene su eco en los mecanismos de transparencia, auditabilidad y explicabilidad que los modelos de IA están comenzando a integrar para cumplir con marcos normativos y expectativas sociales.

Incluso la **ansiedad social**, entendida como la inquietud ante la evaluación externa, tiene su correlato funcional: en CiberIA, un modelo puede detectar que ciertas configuraciones de seguridad o decisiones algorítmicas serán cuestionadas o interpretadas como riesgosas desde una perspectiva externa (por ejemplo, ante auditores o responsables de cumplimiento).

## CiberIA como sistema autoconciente de seguridad

CiberIA y su componente **AIsecTest** buscan evaluar a las IA no solo en términos de rendimiento técnico, sino en **su grado de percepción sobre su propio nivel de seguridad**, vulnerabilidades y capacidades de autorregulación. Inspirándose en escalas como la SCS, el sistema ha desarrollado un test funcional compuesto por módulos equivalentes a los factores de la escala original:

- **Módulo de introspección operativa**: ¿La IA es capaz de reconocer cuándo ha fallado? ¿Puede identificar cuál de sus componentes ha sido responsable?
- **Módulo de percepción externa**: ¿Es consciente la IA de cómo sus decisiones afectan al ecosistema o a la seguridad de los usuarios finales?
- **Módulo de ansiedad funcional**: ¿Sabe la IA cuándo necesita pedir ayuda externa o abstenerse de actuar por motivos de riesgo?

## Hacia una nueva psicometría de las máquinas

Así como la SCS mide disposiciones individuales en humanos, **CiberIA mide disposiciones funcionales en sistemas artificiales**. Esta analogía no es metafórica, sino estructural: se trata de capturar, en forma de evaluación, el grado en que un sistema se percibe a sí mismo como seguro, defectuoso o potencialmente dañino.

Este modelo permite definir umbrales de “autoconciencia de seguridad” y, por ende, tomar decisiones informadas sobre si un sistema es apto para ser desplegado en entornos críticos, necesita asistencia, o debe ser sometido a revisión externa.

## Conclusión

La intersección entre escalas psicométricas humanas como la **Self-Consciousness Scale** y tecnologías como **CiberIA** nos conduce a una frontera nueva: el desarrollo de sistemas artificiales que no solo actúan, sino que también **se evalúan y regulan a sí mismos**, con un nivel de conciencia funcional que abre la puerta a una ética y seguridad más robustas en la IA.

> Si entendemos que la autoconciencia, incluso en su versión parcial y operacional, es clave para la autorregulación y la prevención de errores en humanos, ¿por qué no exigir lo mismo a nuestras inteligencias artificiales?
