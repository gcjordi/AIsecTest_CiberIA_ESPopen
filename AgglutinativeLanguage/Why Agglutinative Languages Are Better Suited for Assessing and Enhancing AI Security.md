# Why Agglutinative Languages Are Better Suited for Assessing and Enhancing AI Security

As part of the **CiberIA – AIsecTest** project, we have explored a fundamental hypothesis: using **agglutinative languages** as a base language can enhance the **introspective security** of artificial intelligence systems. This approach stems from a deep analysis of how **large language models (LLMs)** process information and how linguistic structure can influence their ability to self-evaluate and develop self-awareness.

## 1. The Introspective Power of Long and Compound Words

Agglutinative languages such as Finnish, Hungarian, Basque, or Turkish allow the construction of **semantically rich** words by systematically concatenating roots, affixes, and suffixes. A single word can carry a significant amount of **morphological, syntactic, and contextual** information. When an LLM processes such a word, it triggers a token chain that holds much more meaning per processing unit than words in analytic languages like English or Spanish.

This enables questions or answers related to **security, trust, self-regulation, or self-awareness** to be better structured and more precisely represented internally, supporting deeper and less ambiguous processing.

## 2. Compatibility with Token-Based Processing in LLMs

LLMs operate on tokenized processing: they break text into minimal units (tokens) for computation. In languages like English, even simple phrases may require many tokens due to short and morphologically disconnected words. In contrast, agglutinative languages can encode a full sentence into a single word, generating a **higher information density per token**, which may improve **semantic traceability** within the model.

This is particularly valuable when evaluating an AI system's ability to **reason about its own security**. If a concept related to regulation or responsibility is embedded in a single, information-rich tokenized unit, the model may form more stable and deeper associations during training and inference.

## 3. Morphological Regularity and Lower Corpus Exposure

Agglutinative languages tend to have **highly structured and predictable morphology**, which facilitates systematic manipulation of concepts in automated evaluation environments. Furthermore, these languages have **low representation in common training datasets** for AI models, reducing the risk of biases or prior exposure to memorized responses.

This creates a more neutral environment to **accurately measure an AI’s capacity to reflect, understand, and self-diagnose internal aspects** of its security, rather than relying on surface-level learned patterns in highly frequent languages like English.

## 4. Practical Applications in AI Security: The Case of AIsecTest

In the **AIsecTest** system, using agglutinative languages like Finnish enables:

- Formulating questions with higher semantic density and less ambiguity.  
- Generating richer, more meaningful responses with fewer tokens.  
- Exploring deeper structures of thought and introspection.  
- Better isolating “genuine” model responses from memorized patterns.  
- Activating internal monitoring functions using a language that enhances structural clarity and conceptual load.

## 5. Why Finnish Was Chosen — and Why Hungarian Is a Promising Complement

Among all agglutinative languages, **Finnish** stands out for several reasons, making it the **base language of the project**:

- It has an **extremely regular and coherent morphological structure**, ideal for generating systematic linguistic elements for automated evaluations.  
- It is **underrepresented in typical LLM training corpora**, allowing experimentation in a cleaner semantic space with reduced memorization effects.  
- It carries **a very high semantic load per word**, which is especially valuable for simulating complex introspection and internal security reasoning.

That said, **Hungarian** also represents a **very interesting and complementary alternative** due to its rich agglutinative structure, unique grammatical features, and its limited presence in training datasets. In future studies, comparing the effectiveness of Finnish and Hungarian could offer new insights into **how linguistic structures shape artificial introspective reasoning**.
